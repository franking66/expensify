Expensify Take-Home Assignment: Design & Implementation

Abstract
This document outlines the design and implementation details of the Expensify take-home assignment.

Part I. Scope of The Assignment

1. Provided Infrastructure
   Four Ubuntu servers prebuilt with core OS images:

   Public IP        Internal IP
   35.90.45.219     172.30.0.162
   54.68.212.0      172.30.0.203
   18.237.177.17    172.30.0.51
   34.219.183.229   172.30.0.247

2. Objectives
   Build a load-balanced and secure environment with the following specifications:

 1) Load Balancing:
    Implement any suitable balancing strategy (e.g., round robin, random, load-based).
    Ensure session stickiness is enabled (i.e., sticky sessions).
    The original client IP must be passed to the backend web servers.
    Forward TCP ports 60000–65000 on the load balancer to port 80 on the web servers.

 2) Monitoring:
    Deploy a Nagios server to monitor the full stack.
    Configure Nagios to issue alerts for both warnings and failures.

 3) User Management:
    Add a user named expensify.
    Grant sudo privileges.
    Set up authentication via SSH public key.

 4) Network Security:
    Configure a single public SSH access point (i.e., a jump server) for administrative access to all other instances.
    Disable or remove all unnecessary and unused ports.




Part II. Design

Load Balancer: HAproxy
This assignment requires Sticky Sessions. Out of the box, HAProxy supports cookie-based sticky sessions. This means it can maintain session persistence effectively, ensuring that a client consistently reaches the same backend server unless it goes down—a feature that might require additional modules or workarounds in Nginx.

In this assignment, load balancer needs to listen on a wide port range (60000–65000), then forward traffic to the web servers on port 80. HAProxy Port Range Binding handles such non-standard setups very well, ensuring a clean and efficient traffic flow.

In addition, HAProxy offers a wide variety of load balancing algorithms (round robin, least connections, source-based, etc.) that can be fine-tuned to the application’s needs. It’s designed to handle high volumes of connections efficiently.

Web Server: Apache
HAProxy and Apache work well together for implementing sticky sessions. Sticky sessions are primarily handled by HAProxy, which can be configured to use methods like cookie-based persistence or source IP-based balancing to ensure that a client consistently reaches the same backend server. Apache itself doesn’t enforce stickiness but can support it by serving static content and, if needed, setting cookies through its modules. Apache is widely available on Ubuntu and can be quickly set up to serve a basic static HTML file, making it ideal for this demonstration. Although Apache uses a process-driven or thread-driven model (using MPMs like prefork or worker), which may lead to higher memory consumption under heavy load, this is not a concern in a small-scale demo environment.




Part III. Implementation

System Assignment:
Public IP       Internal IP     Role           Hostname
35.90.45.219    172.30.0.162    Web Server A   web-server-a
54.68.212.0     172.30.0.203    Web Server B   web-server-b
18.237.177.17   172.30.0.51     Load Balancer  load-balancer
34.219.183.229  172.30.0.247    Nagios Server  nagios-host

Change server name accordingly:

Apache Installation:
Apache was installed, and configured using root UID on two servers.  “www-data” was created as a dedicated application account for Apache to run as. Created /var/www/html/index.html containing "Server A" or "B" depends on the server assignment above. Set owner and permission accordingly.

HAproxy Installation:
HAProxy was configured to accept traffic on port 80 and also a range of ports from 60000 to 65000. It forwards these incoming requests to two web servers in a round-robin fashion, maintains sticky sessions (meaning repeated requests from the same client are directed to the same backend server), and preserves the original client's IP address for logging purposes.

     HAproxy.conf Breakdown:

     # default settings; no changes are made
     defaults
         log     global
         mode    http
         option  httplog
         option  dontlognull
         option  forwardfor
         timeout connect 5000
         timeout client  50000
         timeout server  50000

     # The following section is for this assignment 
     frontend web_front
         bind *:80
         bind *:60000-65000
         default_backend web_back


         # frontend web_front:
         # Defines the frontend listener named web_front that accepts incoming requests.
         # HAProxy listens on port 80 on all network interfaces for standard HTTP traffic.
         # Additionally, binds HAProxy to a large custom port range (60000–65000). 
         # Any traffic hitting these ports is internally forwarded to port 80 of backend servers.


     backend web_back
         balance roundrobin
         cookie SRV insert indirect nocache
         server websrv_a 172.30.0.162:80 check cookie A
         server websrv_b 172.30.0.203:80 check cookie B
        
         # Requests are distributed in a simple round-robin manner among web servers.
         # HAProxy inserts a cookie named SRV into client responses to enable sticky sessions. Clients
         # returning with this cookie will be routed to the same backend server repeatedly, ensuring session 
         # persistence.
         # insert: HAProxy injects the cookie automatically.
         # indirect: Clients can't manipulate cookie directly; it references backend servers indirectly, 
         # enhancing security.
         # Enables health checks on the servers. HAProxy periodically verifies server availability and marks 
         # servers as offline automatically if unresponsive.
         # Identifies each backend server uniquely, associating them with the SRV cookie for sticky sessions.
     

Nagios Installation:

Why use Nagios binary--it allows me to always install the very latest version of Nagios with all the newest features and bug fixes. It also gives me complete control over the compilation process, allowing me to enable or disable specific features and optimize the build for my environment.

Why need Apache2--while the Nagios core monitoring engine doesn't inherently require Apache2, its essential and widely used web interface relies on a web server to function and be accessible to users through a web browser. Apache2 is a common and well-supported choice for this purpose on Ubuntu and other Linux distribution.

Nagios user account
The installation requires root privileges, the Nagios core process itself is typically configured to run under a dedicated, less privileged user account (nagios in our case) for security reasons. This principle of least privilege helps to limit the potential damage if the Nagios process were to be compromised.

Problems:
     The following command failed:
     sudo ./configure --with-httpd-conf=/etc/apache2/sites-enabled
     error: Cannot find ssl headers
     
     Solution: add missing lib
          sudo apt install libssl-dev -y
          make clean
          make install
          

Creating Public Access Server (nagios-host):
Nagios daemon runs on nagios-host and requires access to other servers. Therefore, nagios-host was designated as the public-facing SSH access point.
Created a private/public key pair for the expensify account on nagios-host (with a passphrase).
The public key was then distributed to the other three servers.
Authorized users with SSH access to nagios-host can then SSH to the other servers as the expensify user.
Ansible is installed on nagios-host for management purposes.

expensify@nagios-host:~$ ansible all  -i expensify.ini -u expensify -m shell -a "id"
web-server-a | CHANGED | rc=0 >>
uid=1001(expensify) gid=1001(expensify) groups=1001(expensify),27(sudo)
load-balancer | CHANGED | rc=0 >>
uid=1001(expensify) gid=1001(expensify) groups=1001(expensify),27(sudo)
nagios-host | CHANGED | rc=0 >>
uid=1002(expensify) gid=1002(expensify) groups=1002(expensify),27(sudo)
web-server-b | CHANGED | rc=0 >>
uid=1001(expensify) gid=1001(expensify) groups=1001(expensify),27(sudo)


Network Access Control Implementation: 
To enhance network security, the iptables firewall, native to the Ubuntu operating system, was implemented to restrict network access according to defined policies. UFW, an command line interface was used for this task. I also wrote an ansible playbook for this task. However, I decided not to use it since I did not have an environment to test my playbook first.

Firewall Strategy Summary:
Server	      SSH Access	         HTTP/HTTPS Access	      All Other Ports
nagios-host	   Internal + Public	   Internal + Public	      Deny
web-server-a	Internal only	      Internal only	         Deny
web-server-b	Internal only	      Internal only	         Deny
load-balancer	Internal only	      Internal only	         Deny


On Nagios Server (nagios-host):
# Set default policies
sudo ufw default deny incoming
sudo ufw default allow outgoing

# Allow SSH, HTTP, and HTTPS from anywhere
sudo ufw allow 22       # SSH from anywhere
sudo ufw allow 80       # HTTP from anywhere
sudo ufw allow 443      # HTTPS from anywhere (optional for this project)

# Enable UFW
sudo ufw enable


On the Other Servers (web-server-a, web-server-b, load-balancer):
# Set default policies
sudo ufw default deny incoming
sudo ufw default allow outgoing

# Allow SSH, HTTP, and HTTPS only from internal network (172.30.0.0/24)
sudo ufw allow from 172.30.0.0/24 to any port 22 proto tcp   # SSH internal only
sudo ufw allow from 172.30.0.0/24 to any port 80 proto tcp     # HTTP internal only
sudo ufw allow from 172.30.0.0/24 to any port 443 proto tcp    # HTTPS internal only

# Enable UFW
sudo ufw enable

Incident Report:
While attempting to secure nagios-host using UFW, the firewall was enabled before any rules were established. This action locked out access to the server, necessitating the deployment of a new instance. Lesson Learned: The proper procedure for using UFW is to first define the necessary firewall rules and then enable the UFW service. 





Part IV. Findings from Post-Installation Testing

1)Misleading Dashboard Display
The dashboard currently displays four servers and an additional entry labeled localhost, which may cause confusion. This occurs because the default Nagios installation includes a predefined configuration for a host named localhost, which refers to the Nagios server itself (i.e., the machine on which Nagios is running). In this setup, both nagios-host and localhost point to the same IP address (172.30.0.247), resulting in redundancy.

Solution:
Eliminated duplicate localhost entries from the Nagios dashboard. The localhost definition was removed or commented out in /usr/local/nagios/etc/objects/localhost.cfg and /usr/local/nagios/etc/servers.cfg.

2)Nagios Alert: "Swap Usage CRITICAL" was observed on the Nagios Dashboard. 
Supporting Evidence: Ansible output indicated a lack of swap configuration on these systems.

[franking@insprionlinux E]$ ansible all  -i expensify.ini -u ubuntu -m shell -a "free -k &&echo ' '" -b -e "ansible_become_pass=$PASS"
web-server-a | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      185904      148864        1216      616800      595048
Swap:             0           0           0

nagios | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      205064       70060        3412      676444      563812
Swap:             0           0           0

load-balancer | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      248428       66284         880      636856      532776
Swap:             0           0           0

web-server-b | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      180368      181164        1216      590036      601992
Swap:             0           0           0

Solution: 
A 1GB swap file was added to the system. This is recommended when total system RAM is less than 4GB.
The ansible playbook: create_1g_swapfile.yml was created for this task.
This playbook initially returned error: couldn't resolve module/action 'ansible.posix.mount'.

   Fix: Using linefile instead of mount
   - name: Ensure /swapfile entry exists in /etc/fstab
     lineinfile:
      path: /etc/fstab
      line: '/swapfile none swap sw 0 0'
      state: present

   Rerun playbook:
   [franking@insprionlinux E]$ ansible-playbook all -i expensify.ini create_1g_swapfile.yml   -u ubuntu  -b -e "ansible_become_pass=$PASS"

   Ansible-playbook completed Successfully

Swap validation:

[franking@insprionlinux E]$ ansible all  -i expensify.ini -u ubuntu -m shell -a "swapon -s &&echo ' '" -b -e "ansible_become_pass=$PASS"
web-server-a | CHANGED | rc=0 >>
Filename                                Type            Size    Used    Priority
/swapfile                               file            1048572 768     -2

nagios | CHANGED | rc=0 >>
Filename                                Type            Size    Used    Priority
/swapfile                               file            1048572 1024    -2

web-server-b | CHANGED | rc=0 >>
Filename                                Type            Size    Used    Priority
/swapfile                               file            1048572 256     -2

load-balancer | CHANGED | rc=0 >>
Filename                                Type            Size    Used    Priority
/swapfile                               file            1048572 256     -2


[franking@insprionlinux E]$ ansible all  -i expensify.ini -u ubuntu -m shell -a "free -k &&echo ' '" -b -e "ansible_become_pass=$PASS"
web-server-a | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      179704       70296        1140      701568      598352
Swap:       1048572         768     1047804

load-balancer | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      243220       63608         780      644740      535344
Swap:       1048572         256     1048316

web-server-b | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      168460       71936        1140      711172      610136
Swap:       1048572         256     1048316

nagios | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:         951568      204548       61608        3388      685412      560476
Swap:       1048572        1024     1047548



[franking@insprionlinux E]$ ansible all  -i expensify.ini -u ubuntu -m shell -a "grep -i swap /etc/fstab&&echo ' '" -b -e "ansible_become_pass=$PASS"
 
load-balancer | CHANGED | rc=0 >>
/swapfile none swap sw 0 0

web-server-a | CHANGED | rc=0 >>
/swapfile none swap sw 0 0

nagios | CHANGED | rc=0 >>
/swapfile none swap sw 0 0

web-server-b | CHANGED | rc=0 >>
/swapfile none swap sw 0 0

3)Nagios service monitoring was limited to nagios-host.
Resolution: Modified servers.cfg to extend service status monitoring to web-server-a, web-server-b, and load-balancer.

4)Initial Monitoring Scope: HAproxy monitoring was limited and did not cover HTTP stickiness or activity within the high port range (60000-65000). 

Solution: The custom Python script check_haproxy_stickiness.py has been implemented to enhance monitoring capabilities. This script checks for the SRV cookie to determine stickiness (OK if found) and reports CRITICAL if stickiness is not functioning (no cookie or connection issues).

   Observed Problem: The check_haproxy_stickiness.py script exhibited a failure to capture Set-Cookie headers. Technical Explanation: This occurs because HTTP responses may be segmented across multiple TCP segments. A single recv() call in the script might not retrieve the complete HTTP header containing the cookie information. curl's behavior of reading until the connection closes or a complete response is received mitigates this.

   Resolution: The check_haproxy_stickiness.py script was modified to implement a loop that continues to receive and process incoming TCP packets until the presence of cookie data is confirmed.

      while True:
         data = sock.recv(4096).decode()
         if not data:
            break
         response += data
      sock.close()



